{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning 03a. Recurrent Neural Nets (RNN)\n",
    "\n",
    "* **Implementation 4a**: RNN from scratch (basics)\n",
    "    * *Source*: Adapted from Dan Britz's tutorial on Language Model with RNN\n",
    "    * *Contribution*:\n",
    "        * Fixed bugs in the source.\n",
    "        * Adapted to sequence labeling task.\n",
    "        * Added annotations.\n",
    "\n",
    "* **Implementation 4b**: RNN with Keras (basics)\n",
    "    * *Source*: My RNN code at https://github.com/suwangcompling/texasdataday2017/blob/master/NER_ATIS.ipynb\n",
    "    * *Contribution*: \n",
    "        * Hopefully clearer pipeline\n",
    "\n",
    "* **Implementation 4c**: RNN with Keras (bidirectional setup + tuning options)\n",
    "    * *Advanced Model*: Bi-LSTM\n",
    "    * *Even advanced-ier Model*: Bi-LSTM-CRF (https://github.com/glample/tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Implementation 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from itertools import chain\n",
    "from spacy.en import English\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "sents = brown.sents()\n",
    "\n",
    "# DATA NORMALIZATION (LEMMATIZATION)\n",
    "\n",
    "# set limit on vocab size (for demo purpose)\n",
    "vocabulary_size = 10000\n",
    "# handle unknown token and corresponding label\n",
    "unknown_token = \"<UNK>\"\n",
    "unknown_label = \"UNK\"\n",
    "# start/end tokens (cf. Jurafsky & Martin on n-gram language models)\n",
    "sentence_start_token = \"<S>\"\n",
    "sentence_end_token = \"</S>\"\n",
    "\n",
    "# load parser\n",
    "parser = English()\n",
    "# lemmatize\n",
    "X, Y = [], []\n",
    "for sent in sents:\n",
    "    words = ' '.join(sent) # list of words -> sent as string, for spacy parser.\n",
    "    parsed_sent = parser(unicode(words))\n",
    "    lemmas = [token.lemma_ for token in parsed_sent]\n",
    "    labels = [token.pos_ for token in parsed_sent] # use spacy's tagging as target.\n",
    "    X.append(lemmas)\n",
    "    Y.append(labels)\n",
    "\n",
    "# build word dictionary for lookup\n",
    "word_freq = nltk.FreqDist(chain(*X))    \n",
    "vocab = word_freq.most_common(vocabulary_size - 1) # [(w,freq)...]. leave 1 slot for <UNK>.\n",
    "i2w = [elem[0] for elem in vocab] + [unknown_token]\n",
    "w2i = {w:i for i,w in enumerate(i2w)}\n",
    "# replace words under frequency cut to <UNK>\n",
    "for i,sent in enumerate(X):\n",
    "    X[i] = [w if w in w2i else unknown_token for w in sent]\n",
    "    \n",
    "# build label dictionary for lookup\n",
    "label_vocab = list(set(chain(*Y)))\n",
    "i2l = [x for x in label_vocab] + [unknown_label]\n",
    "l2i = dict([(l,i) for i,l in enumerate(i2l)])\n",
    "label_vocabulary_size = len(l2i)\n",
    "for i,labels in enumerate(Y):\n",
    "    Y[i] = [l for l in labels]\n",
    "\n",
    "# one-hot encoding: word/label -> word/label index\n",
    "train_test_split = (int)(len(X)*0.95)\n",
    "X_encoded = np.asarray([[w2i[w] for w in sent] for sent in X])\n",
    "Y_encoded = np.asarray([[l2i[unknown_label] if X[i][j]==unknown_token else l2i[l]\n",
    "                       for j,l in enumerate(labels)] for i,labels in enumerate(Y)])\n",
    "X_train = X_encoded[:train_test_split]\n",
    "Y_train = Y_encoded[:train_test_split]\n",
    "X_test = X_encoded[train_test_split:]\n",
    "Y_test = Y_encoded[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN POS-TAGGING MODEL\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax transformation.\n",
    "    \n",
    "    Arguments:\n",
    "    x: List of vocab-index-encoded words.\n",
    "    \n",
    "    Returns: Softmax transformed x.\n",
    "    \"\"\"\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "class RNN:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # input -> hidden projection matrix\n",
    "        self.U = np.random.uniform(-np.sqrt(1./input_dim), np.sqrt(1./input_dim), (hidden_dim, input_dim))\n",
    "        # hidden -> output projection matrix\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (output_dim, hidden_dim))\n",
    "        # hidden (current t) -> hidden (next t) projection matrix\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        \"\"\"\n",
    "        Feedforward step, feed input through RNN to obtain output.\n",
    "        \n",
    "        Arguments:\n",
    "        x: Input vector, a list of vocab-index-encoded words.\n",
    "        \n",
    "        Returns output vector.\n",
    "        \"\"\"\n",
    "        T = len(x)\n",
    "        s = np.zeros((T+1, self.hidden_dim)) # add 1 more time step for initial state.\n",
    "        s[-1] = np.zeros(self.hidden_dim) # initialize initial state to 0.\n",
    "        o = np.zeros((T, self.output_dim))\n",
    "        # Elman's RNN \n",
    "        #   link: https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks)\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        POS-tag a sentence.\n",
    "        \n",
    "        Arguments: \n",
    "        x: Input vector, a list of vocab-index-encoded words.\n",
    "        \n",
    "        Returns vocab-index-encoded labels.\n",
    "        \"\"\"\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute cross entropy cost.\n",
    "        \n",
    "        Arguments:\n",
    "        x: Input vector, a list of vocab-index-encoded words.\n",
    "        y: True label vector, vocab-index-encoded labels.\n",
    "        \n",
    "        Returns average cost over N (data size) data.\n",
    "        \"\"\"\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        L = 0\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))        \n",
    "        return L / N\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation Through Time (BPTT) algorithm.\n",
    "        \n",
    "        Arguments:\n",
    "        x: Input vector, a list of vocab-index-encoded words.\n",
    "        y: True label vector, vocab-index-encoded labels.  \n",
    "        \n",
    "        Returns gradients of matrices U, V, W.\n",
    "        \"\"\"\n",
    "        T = len(y)\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # empty containers for gradients.\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        # cf. Mike Nielsen's deep learning book for detailed derivation (ch3 on backprop).\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # for each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            # a shortcut for computing gradient (for the more intuitive form, cf. Nielsen book: eq. BP3).\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2)) # (1 - (s[t] ** 2)): derivative of tanh.\n",
    "            # backpropagation through time (for at most bptt_truncate steps)\n",
    "            #   ONLY DIFFERENT FROM STANDARD BACKPROP: accumulate/sum gradients at each time step.\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def update(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights by gradients.\n",
    "        \n",
    "        Arguments:\n",
    "        x: Input vector, a list of vocab-index-encoded words.\n",
    "        y: True label vector, vocab-index-encoded labels. \n",
    "        \"\"\"\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \"\"\"\n",
    "        Evaluate accuracy.\n",
    "        \n",
    "        Arguments:\n",
    "        X_test: List of lists of vocab-index-encoded words.\n",
    "        Y_test: List of lists of vocab-index-encoded labels. \n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for x,y in zip(X_test, Y_test):\n",
    "            y_pred = self.predict(x)\n",
    "            y_true = np.asarray(y)\n",
    "            accuracies.append(accuracy_score(y_true, y_pred))\n",
    "        print \"Average Accuracy:\", np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-14 21:54:49: Loss after num_examples_seen=0 epoch=0: 2.772439\n",
      "2017-02-14 21:55:40: Loss after num_examples_seen=5000 epoch=1: 0.513475\n",
      "2017-02-14 21:56:31: Loss after num_examples_seen=10000 epoch=2: 0.307118\n",
      "2017-02-14 21:57:23: Loss after num_examples_seen=15000 epoch=3: 0.230946\n",
      "2017-02-14 21:58:11: Loss after num_examples_seen=20000 epoch=4: 0.191842\n",
      "Average Accuracy: 0.911805363531\n"
     ]
    }
   ],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT\n",
    "#   batch_size=1. simpler to batch with Keras (later).\n",
    "\n",
    "learning_rate = 0.005\n",
    "epochs = 5\n",
    "verbose_freq = 1 # frequency of current loss report.\n",
    "\n",
    "rnn = RNN(vocabulary_size, label_vocabulary_size)\n",
    "\n",
    "# train to demo on partial data.\n",
    "X_train_sub = X_train[:5000]\n",
    "Y_train_sub = Y_train[:5000]\n",
    "\n",
    "losses = []\n",
    "num_examples_seen = 0\n",
    "for epoch in range(epochs):\n",
    "    if (epoch % verbose_freq == 0):\n",
    "        loss = rnn.calculate_loss(X_train_sub, Y_train_sub)\n",
    "        losses.append((num_examples_seen, loss))\n",
    "        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "        # learning rate decay: adjust the learning rate if loss increases\n",
    "        if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "            learning_rate = learning_rate * 0.5 \n",
    "            print \"Setting learning rate to %f\" % learning_rate\n",
    "        sys.stdout.flush()\n",
    "    # for each training example\n",
    "    for i in range(len(Y_train_sub)):\n",
    "        rnn.update(X_train_sub[i], Y_train_sub[i], learning_rate)\n",
    "        num_examples_seen += 1\n",
    "\n",
    "# EVALUATION\n",
    "rnn.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Implementation 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, os, random\n",
    "os.environ['KERAS_BACKEND']='tensorflow' \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Activation, TimeDistributed\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "path = \"/Users/jacobsw/Desktop/WORK/OJO/NER_PRESENTATION/DATA/atis.pkl\"\n",
    "train_triple, valid_triple, test_triple, dicts = pickle.load(open(path, 'rb'))\n",
    "\n",
    "X_train, Y_train = train_triple[0], train_triple[2]\n",
    "X_valid, Y_valid = valid_triple[0], valid_triple[2]\n",
    "X_test, Y_test = test_triple[0], test_triple[2]\n",
    "\n",
    "l2i = dicts['labels2idx']\n",
    "w2i = dicts['words2idx']\n",
    "i2l = {i:l for l,i in l2i.iteritems()}\n",
    "i2w = {i:w for w,i in w2i.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET CONFIGS\n",
    "\n",
    "vocab_size = len(w2i)\n",
    "label_size = len(l2i)\n",
    "emb_size = 100\n",
    "hidden_size = 100\n",
    "\n",
    "num_epochs = 20\n",
    "valid_freq = 1000\n",
    "valid_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dim_transform_x(x):\n",
    "    \"\"\"\n",
    "    Reshape an x data point to [batch_size, length].\n",
    "    \n",
    "    Arguments:\n",
    "    x: Single x data point.\n",
    "    \n",
    "    Returns reshaped data point.\n",
    "    \"\"\"\n",
    "    return np.asarray([x])\n",
    "\n",
    "def dim_transform_y(y):\n",
    "    \"\"\"\n",
    "    Reshape an y data point to [batch_size, length, label_size] (in binarized representation).\n",
    "    \n",
    "    Arguments:\n",
    "    y: Single y data point.\n",
    "    \n",
    "    Returns reshaped data point.\n",
    "    \"\"\"\n",
    "    return to_categorical(np.asarray(y)[:,np.newaxis], nb_classes=label_size)[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINSFORM DATA TO FIT INPUT REQUIREMENTS\n",
    "\n",
    "X_train, X_valid, X_test = map(dim_transform_x, X_train), map(dim_transform_x, X_valid), map(dim_transform_x, X_test)\n",
    "Y_train, Y_valid, Y_test = map(dim_transform_y, Y_train), map(dim_transform_y, Y_valid), map(dim_transform_y, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, None, 100)     57200       embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                    (None, None, 100)     80400       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_6 (TimeDistribut (None, None, 127)     12827       lstm_7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, None, 127)     0           timedistributed_6[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 150427\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BUILD COMPUTATIONAL GRAPH\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(input_dim=vocab_size, output_dim=emb_size))\n",
    "rnn.add(LSTM(output_dim=hidden_size, activation='relu', return_sequences=True))\n",
    "rnn.add(TimeDistributed(Dense(output_dim=label_size)))\n",
    "rnn.add(Activation('softmax'))\n",
    "rnn.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Cost/Accuracy at Iteration 1000 : 0.630596980916 0.860808443064\n",
      "Validation Cost/Accuracy at Iteration 2000 : 0.404342949591 0.911917387688\n",
      "Validation Cost/Accuracy at Iteration 3000 : 0.319398777756 0.9242008791\n",
      "Validation Cost/Accuracy at Iteration 4000 : 0.241086330384 0.939273726382\n",
      "Validation Cost/Accuracy at Iteration 5000 : 0.193205771447 0.957866637992\n",
      "Validation Cost/Accuracy at Iteration 6000 : 0.120419648209 0.971302836597\n",
      "Validation Cost/Accuracy at Iteration 7000 : 0.143774769793 0.966408751208\n",
      "Validation Cost/Accuracy at Iteration 8000 : 0.0945306488611 0.976764610591\n",
      "Validation Cost/Accuracy at Iteration 9000 : 0.0704847894763 0.980819010238\n",
      "Validation Cost/Accuracy at Iteration 10000 : 0.0993882891076 0.980816777993\n",
      "Validation Cost/Accuracy at Iteration 11000 : 0.070867968236 0.980805333555\n",
      "Validation Cost/Accuracy at Iteration 12000 : 0.0725929829256 0.980705998394\n",
      "Validation Cost/Accuracy at Iteration 13000 : 0.0701369878041 0.978846239016\n",
      "Validation Cost/Accuracy at Iteration 14000 : 0.055321096814 0.985591687123\n",
      "Validation Cost/Accuracy at Iteration 15000 : 0.082208160485 0.98152576182\n",
      "Validation Cost/Accuracy at Iteration 16000 : 0.0932055813751 0.979833971584\n",
      "Validation Cost/Accuracy at Iteration 17000 : 0.097235277678 0.98339066227\n",
      "Validation Cost/Accuracy at Iteration 18000 : 0.0744492412313 0.979834560048\n",
      "Validation Cost/Accuracy at Iteration 19000 : 0.149206025631 0.96492680565\n",
      "Validation Cost/Accuracy at Iteration 20000 : 0.0972592044409 0.986194611976\n",
      "Validation Cost/Accuracy at Iteration 21000 : 0.0991858438673 0.982092270689\n",
      "Validation Cost/Accuracy at Iteration 22000 : 0.0983152137044 0.98189093831\n",
      "Validation Cost/Accuracy at Iteration 23000 : 0.0776311035459 0.981572089947\n",
      "Validation Cost/Accuracy at Iteration 24000 : 0.0822273398102 0.991286680912\n",
      "Validation Cost/Accuracy at Iteration 25000 : 0.0475360258085 0.981868856144\n",
      "Validation Cost/Accuracy at Iteration 26000 : 0.0620118604078 0.984955646692\n",
      "Validation Cost/Accuracy at Iteration 27000 : 0.0941353762938 0.981714716656\n",
      "Validation Cost/Accuracy at Iteration 28000 : 0.0432620983387 0.989858604845\n",
      "Validation Cost/Accuracy at Iteration 29000 : 0.134825625153 0.975181401931\n",
      "Validation Cost/Accuracy at Iteration 30000 : 0.0676169956768 0.985461650922\n",
      "Validation Cost/Accuracy at Iteration 31000 : 0.0941777639801 0.979448412698\n",
      "Validation Cost/Accuracy at Iteration 32000 : 0.0983376825217 0.982498677249\n",
      "Validation Cost/Accuracy at Iteration 33000 : 0.0787985636896 0.985039990647\n",
      "Validation Cost/Accuracy at Iteration 34000 : 0.119217622772 0.97979540884\n",
      "Validation Cost/Accuracy at Iteration 35000 : 0.0867029541723 0.985737429237\n",
      "Validation Cost/Accuracy at Iteration 36000 : 0.134242145902 0.979425007182\n",
      "Validation Cost/Accuracy at Iteration 37000 : 0.200994423362 0.969505622452\n",
      "Validation Cost/Accuracy at Iteration 38000 : 0.0723596818541 0.986741180603\n",
      "Validation Cost/Accuracy at Iteration 39000 : 0.0683249885629 0.98984563785\n",
      "Validation Cost/Accuracy at Iteration 40000 : 0.131889400835 0.982657325131\n",
      "Validation Cost/Accuracy at Iteration 41000 : 0.095033911442 0.982094445552\n",
      "Validation Cost/Accuracy at Iteration 42000 : 0.0930074410239 0.98365492958\n",
      "Validation Cost/Accuracy at Iteration 43000 : 0.0718948281366 0.987338141026\n",
      "Validation Cost/Accuracy at Iteration 44000 : 0.082216739769 0.983591117216\n",
      "Validation Cost/Accuracy at Iteration 45000 : 0.119315527093 0.975061091686\n",
      "Validation Cost/Accuracy at Iteration 46000 : 0.128825071965 0.980573260073\n",
      "Validation Cost/Accuracy at Iteration 47000 : 0.0433949017442 0.992088402825\n",
      "Validation Cost/Accuracy at Iteration 48000 : 0.101449182128 0.984457912458\n",
      "Validation Cost/Accuracy at Iteration 49000 : 0.131200355529 0.981513986014\n",
      "Validation Cost/Accuracy at Iteration 50000 : 0.115522700698 0.977839225991\n",
      "Validation Cost/Accuracy at Iteration 51000 : 0.0767026057963 0.985096350509\n",
      "Validation Cost/Accuracy at Iteration 52000 : 0.116610531507 0.98183022533\n",
      "Validation Cost/Accuracy at Iteration 53000 : 0.0419980216608 0.987815161891\n",
      "Validation Cost/Accuracy at Iteration 54000 : 0.134853158726 0.981954885392\n",
      "Validation Cost/Accuracy at Iteration 55000 : 0.0696760871645 0.988216123359\n",
      "Validation Cost/Accuracy at Iteration 56000 : 0.0920862521988 0.983744053762\n",
      "Validation Cost/Accuracy at Iteration 57000 : 0.100844216167 0.98654516317\n",
      "Validation Cost/Accuracy at Iteration 58000 : 0.0599914464432 0.992055860806\n",
      "Validation Cost/Accuracy at Iteration 59000 : 0.198353185089 0.976215966051\n",
      "Validation Cost/Accuracy at Iteration 60000 : 0.0949643262541 0.987106120932\n",
      "Validation Cost/Accuracy at Iteration 61000 : 0.0951792011279 0.982720013646\n",
      "Validation Cost/Accuracy at Iteration 62000 : 0.0933282926692 0.98275528638\n",
      "Validation Cost/Accuracy at Iteration 63000 : 0.199296649828 0.975680037555\n",
      "Validation Cost/Accuracy at Iteration 64000 : 0.0901573026669 0.983120879121\n",
      "Validation Cost/Accuracy at Iteration 65000 : 0.112411752882 0.977377996867\n",
      "Validation Cost/Accuracy at Iteration 66000 : 0.126352889169 0.985147839519\n",
      "Validation Cost/Accuracy at Iteration 67000 : 0.107661231815 0.984577043871\n",
      "Validation Cost/Accuracy at Iteration 68000 : 0.199220192579 0.973582958708\n",
      "Validation Cost/Accuracy at Iteration 69000 : 0.0476702494758 0.992804511278\n",
      "Validation Cost/Accuracy at Iteration 70000 : 0.092819010879 0.986762192056\n",
      "Validation Cost/Accuracy at Iteration 71000 : 0.109916900798 0.973445250828\n",
      "Validation Cost/Accuracy at Iteration 72000 : 0.0727542731327 0.985527912591\n",
      "Validation Cost/Accuracy at Iteration 73000 : 0.215869725749 0.97889209973\n",
      "Validation Cost/Accuracy at Iteration 74000 : 0.115757844987 0.982387756035\n",
      "Validation Cost/Accuracy at Iteration 75000 : 0.122396487441 0.97976747558\n",
      "Validation Cost/Accuracy at Iteration 76000 : 0.101020101777 0.983943320568\n",
      "Validation Cost/Accuracy at Iteration 77000 : 0.118113378931 0.981601037851\n",
      "Validation Cost/Accuracy at Iteration 78000 : 0.106839086169 0.98543582398\n",
      "Validation Cost/Accuracy at Iteration 79000 : 0.0892001500839 0.98744447192\n",
      "Test Cost/Accuracy: 0.285456630415 0.96842926734\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "num_iters = 0\n",
    "\n",
    "for i in xrange(num_epochs):\n",
    "    for t in xrange(len(X_train)):\n",
    "        rnn.train_on_batch(X_train[t], Y_train[t])\n",
    "        num_iters += 1\n",
    "        if num_iters%valid_freq==0:\n",
    "            valid_ids = random.sample(range(len(X_valid)), valid_size)\n",
    "            valid_costs, valid_accs = [], []\n",
    "            for v in valid_ids:\n",
    "                valid_cost, valid_acc = rnn.evaluate(X_valid[v], Y_valid[v], verbose=0)\n",
    "                valid_costs.append(valid_cost); valid_accs.append(valid_acc)\n",
    "            print \"Validation Cost/Accuracy at Iteration\", num_iters, \":\", np.mean(valid_costs), np.mean(valid_accs) \n",
    "\n",
    "# EVALUATE    \n",
    "\n",
    "test_costs, test_accs = [], []\n",
    "for e in xrange(len(X_test)):\n",
    "    test_cost, test_acc = rnn.evaluate(X_test[e], Y_test[e], verbose=0)\n",
    "    test_costs.append(test_cost); test_accs.append(test_acc)\n",
    "print \"Test Cost/Accuracy:\", np.mean(test_costs), np.mean(test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Implementation 4c\n",
    "\n",
    "* Tuning:\n",
    "    * Regularization: Dropout\n",
    "    * Learning: Learning Decay\n",
    "\n",
    "**NB**: Load data using code in Impl. 4a first. \n",
    "\n",
    "**NB**: Providing syntax. Tuning needed to make this thing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(w2i)\n",
    "label_size = len(l2i)\n",
    "emb_size = 100\n",
    "hidden_size = 100\n",
    "\n",
    "num_epochs = 20\n",
    "valid_freq = 1000\n",
    "valid_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_12 (Embedding)         (None, None, 100)     57200       embedding_input_12[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional)  (None, None, 200)     160800      embedding_12[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, None, 200)     0           bidirectional_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional)  (None, None, 200)     240800      dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, None, 200)     0           bidirectional_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_10 (TimeDistribu (None, None, 127)     25527       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, None, 127)     0           timedistributed_10[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 484327\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BUILD COMPUTATIONAL GRAPH\n",
    "\n",
    "bilstm = Sequential()\n",
    "bilstm.add(Embedding(input_dim=vocab_size, output_dim=emb_size))\n",
    "bilstm.add(Bidirectional(LSTM(output_dim=hidden_size, activation='relu', return_sequences=True)))\n",
    "bilstm.add(Dropout(p=0.5))\n",
    "bilstm.add(Bidirectional(LSTM(output_dim=hidden_size, activation='relu', return_sequences=True)))\n",
    "bilstm.add(Dropout(p=0.5))\n",
    "bilstm.add(TimeDistributed(Dense(output_dim=label_size)))\n",
    "bilstm.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam(decay=0.9)\n",
    "bilstm.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "num_iters = 0\n",
    "\n",
    "for i in xrange(num_epochs):\n",
    "    for t in xrange(len(X_train)):\n",
    "        bilstm.train_on_batch(X_train[t], Y_train[t])\n",
    "        num_iters += 1\n",
    "        if num_iters%valid_freq==0:\n",
    "            valid_ids = random.sample(range(len(X_valid)), valid_size)\n",
    "            valid_costs, valid_accs = [], []\n",
    "            for v in valid_ids:\n",
    "                valid_cost, valid_acc = bilstm.evaluate(X_valid[v], Y_valid[v], verbose=0)\n",
    "                valid_costs.append(valid_cost); valid_accs.append(valid_acc)\n",
    "            print \"Validation Cost/Accuracy at Iteration\", num_iters, \":\", np.mean(valid_costs), np.mean(valid_accs)\n",
    "    \n",
    "# EVALUATE    \n",
    "\n",
    "test_costs, test_accs = [], []\n",
    "for e in xrange(len(X_test)):\n",
    "    test_cost, test_acc = bilstm.evaluate(X_test[e], Y_test[e], verbose=0)\n",
    "    test_costs.append(test_cost); test_accs.append(test_acc)\n",
    "print \"Test Cost/Accuracy:\", np.mean(test_costs), np.mean(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
