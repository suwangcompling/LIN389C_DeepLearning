{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning 03b. Recurrent Neural Nets (RNN)\n",
    "\n",
    "**NB**: This is a simple demo. Hyperparams are not tuned for optimal performance.\n",
    "\n",
    "* **Implementation 4d**: RNN with Tensorflow (simple RNN & LSTM with naive padding)\n",
    "    * *Source*: \n",
    "        * Erik Hallstr&ouml;m's blog: https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185#.qg4y5kgbq.\n",
    "        * R2RT's blog: http://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html.\n",
    "    * *Sections*:\n",
    "        * Simple RNN\n",
    "        * LSTM\n",
    "        * Bi-LSTM\n",
    "        * Multi-Layer LSTM\n",
    "    * *Contribution*:\n",
    "        * Both bloggers are brilliant, but neither did a simple POS-tagger example for non-technical linguists.\n",
    "\n",
    "* **Implementation 4e**: RNN with Tensorflow (dynamic rnn)\n",
    "    * *Source*: as above.\n",
    "    * *Sections*:\n",
    "        * Dynamic Multi-Layer LSTM\n",
    "        * Dynamic Bi-LSTM\n",
    "    * *Contribution*: \n",
    "        * Add dynamic fitting to sequences with varied lengths.\n",
    "        * Add bi-directional architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import brown\n",
    "from itertools import chain\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Implementation 4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Simple RNN\n",
    "\n",
    "* Sample Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "sents = brown.sents()\n",
    "\n",
    "# DATA NORMALIZATION (LEMMATIZATION)\n",
    "\n",
    "# set limit on vocab size (for demo purpose)\n",
    "vocabulary_size = 10000\n",
    "# handle unknown token and corresponding label\n",
    "pad_token = \"<PAD>\"\n",
    "pad_label = \"PAD\"\n",
    "unknown_token = \"<UNK>\"\n",
    "unknown_label = \"UNK\"\n",
    "# start/end tokens (cf. Jurafsky & Martin on n-gram language models)\n",
    "sentence_start_token = \"<S>\"\n",
    "sentence_end_token = \"</S>\"\n",
    "\n",
    "# load parser\n",
    "parser = English()\n",
    "# lemmatize\n",
    "X, Y = [], []\n",
    "for sent in sents:\n",
    "    words = ' '.join(sent) # list of words -> sent as string, for spacy parser.\n",
    "    parsed_sent = parser(unicode(words))\n",
    "    lemmas = [token.lemma_ for token in parsed_sent]\n",
    "    labels = [token.pos_ for token in parsed_sent] # use spacy's tagging as target.\n",
    "    X.append(lemmas)\n",
    "    Y.append(labels)\n",
    "\n",
    "# build word dictionary for lookup\n",
    "word_freq = nltk.FreqDist(chain(*X))    \n",
    "vocab = word_freq.most_common(vocabulary_size - 1) # [(w,freq)...]. leave 1 slot for <UNK>.\n",
    "i2w = [pad_token] + [elem[0] for elem in vocab] + [unknown_token]\n",
    "w2i = {w:i for i,w in enumerate(i2w)}\n",
    "# replace words under frequency cut to <UNK>\n",
    "for i,sent in enumerate(X):\n",
    "    X[i] = [w if w in w2i else unknown_token for w in sent]\n",
    "    \n",
    "# build label dictionary for lookup\n",
    "label_vocab = list(set(chain(*Y)))\n",
    "i2l = [pad_label] + [x for x in label_vocab] + [unknown_label]\n",
    "l2i = dict([(l,i) for i,l in enumerate(i2l)])\n",
    "label_vocabulary_size = len(l2i)\n",
    "for i,labels in enumerate(Y):\n",
    "    Y[i] = [l for l in labels]\n",
    "\n",
    "# one-hot encoding: word/label -> word/label index\n",
    "train_test_split = (int)(len(X)*0.95)\n",
    "X_encoded = np.asarray([[w2i[w] for w in sent] for sent in X])\n",
    "Y_encoded = np.asarray([[l2i[unknown_label] if X[i][j]==unknown_token else l2i[l]\n",
    "                       for j,l in enumerate(labels)] for i,labels in enumerate(Y)])\n",
    "X_train = X_encoded[:train_test_split]\n",
    "Y_train = Y_encoded[:train_test_split]\n",
    "X_test = X_encoded[train_test_split:]\n",
    "Y_test = Y_encoded[train_test_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAKE DATA ITERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 4943, 621, 2075, 1658, 45, 1805, 34, 1568,...</td>\n",
       "      <td>[6, 5, 5, 5, 5, 15, 5, 6, 2, 3, 5, 10, 14, 2, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1658, 533, 45, 9, 378, 17, 188, 10000, 11,...</td>\n",
       "      <td>[6, 2, 1, 15, 3, 2, 9, 2, 16, 3, 6, 5, 5, 5, 9...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1888, 17, 2056, 378, 1658, 10, 4, 509, 25,...</td>\n",
       "      <td>[6, 5, 9, 5, 2, 2, 15, 15, 15, 3, 5, 5, 5, 5, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[15, 76, 8, 1683, 5693, 5, 97, 299, 4, 405, 14...</td>\n",
       "      <td>[9, 1, 6, 14, 2, 3, 14, 2, 15, 15, 9, 9, 6, 2,...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1658, 45, 16, 32, 112, 11, 111, 5, 1984, 2...</td>\n",
       "      <td>[6, 2, 15, 4, 15, 15, 3, 14, 3, 5, 10, 2, 13, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  \\\n",
       "0  [1, 4943, 621, 2075, 1658, 45, 1805, 34, 1568,...   \n",
       "1  [1, 1658, 533, 45, 9, 378, 17, 188, 10000, 11,...   \n",
       "2  [1, 1888, 17, 2056, 378, 1658, 10, 4, 509, 25,...   \n",
       "3  [15, 76, 8, 1683, 5693, 5, 97, 299, 4, 405, 14...   \n",
       "4  [1, 1658, 45, 16, 32, 112, 11, 111, 5, 1984, 2...   \n",
       "\n",
       "                                                   Y  seq_len  \n",
       "0  [6, 5, 5, 5, 5, 15, 5, 6, 2, 3, 5, 10, 14, 2, ...       26  \n",
       "1  [6, 2, 1, 15, 3, 2, 9, 2, 16, 3, 6, 5, 5, 5, 9...       47  \n",
       "2  [6, 5, 9, 5, 2, 2, 15, 15, 15, 3, 5, 5, 5, 5, ...       41  \n",
       "3  [9, 1, 6, 14, 2, 3, 14, 2, 15, 15, 9, 9, 6, 2,...       37  \n",
       "4  [6, 2, 15, 4, 15, 15, 3, 14, 3, 5, 10, 2, 13, ...       25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PANDA DATAFRAME (sequence lenth for later: dynamic rnn)\n",
    "\n",
    "df_train = pd.DataFrame({'X':X_train, 'Y':Y_train})\n",
    "df_train['seq_len'] = map(lambda x:len(x), df_train['X'])\n",
    "\n",
    "df_test = pd.DataFrame({'X':X_test, 'Y':Y_test})\n",
    "df_test['seq_len'] = map(lambda x:len(x), df_test['X'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sequences:\n",
      "0                     [15, 26, 32, 21, 119, 63, 14, 3]\n",
      "1    [27, 4, 21, 851, 7, 94, 394, 5, 3733, 22, 1, 1...\n",
      "2    [622, 12, 496, 118, 1, 10000, 11, 1, 10000, 62...\n",
      "Name: X, dtype: object\n",
      "Y sequences:\n",
      "0                           [9, 4, 15, 1, 15, 1, 9, 9]\n",
      "1    [6, 15, 1, 14, 3, 6, 2, 3, 2, 3, 6, 2, 3, 6, 1...\n",
      "2    [1, 4, 15, 3, 6, 16, 3, 6, 16, 15, 15, 12, 5, ...\n",
      "Name: Y, dtype: object\n",
      "sequence lengths:\n",
      "0     8\n",
      "1    17\n",
      "2    19\n",
      "Name: seq_len, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# BASE ITERATOR\n",
    "\n",
    "class DataIterator():\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        # sample 100% with different index, but not add new index column\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True) \n",
    "        self.cursor = 0\n",
    "    \n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        return res['X'], res['Y'], res['seq_len']\n",
    "    \n",
    "# EXAMPLE\n",
    "d = DataIterator(df_train)\n",
    "t = d.next_batch(3)\n",
    "\n",
    "print 'X sequences:\\n', t[0]\n",
    "print 'Y sequences:\\n', t[1]\n",
    "print 'sequence lengths:\\n', t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sequences:\n",
      "[[  26   10  246   18 7223  640   77  115    2  185    1 2443   77    1\n",
      "  4180   99  101    2  322   18    1  297   17  527   17  761 2716   13\n",
      "    95 1419    5 6961   22 1659  386    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Y sequences:\n",
      "[[ 4 15 15  3  5 12  2  1  9  3  6  2  2  6  5 15  4  9  3  3  6 12  9 12\n",
      "   9  2  2  3 14  2  3  2  3  5  5  9  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "sequence lengths:\n",
      "0    36\n",
      "Name: seq_len, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# PADDED ITERATOR\n",
    "\n",
    "max_len = max(max(df_train['seq_len']), max(df_test['seq_len'])) # longest sent in corpus.\n",
    "\n",
    "class FullPaddedDataIterator(DataIterator):\n",
    "    \n",
    "    def next_batch(self, n):\n",
    "        \n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        \n",
    "        maxlen = max_len \n",
    "        x = np.zeros([n, maxlen], dtype=np.int32) # empty container.\n",
    "        y = np.zeros([n, maxlen], dtype=np.int32)\n",
    "        for i, (x_i,y_i) in enumerate(zip(x,y)):\n",
    "            x_i[:res['seq_len'].values[i]] = res['X'].values[i] # only fill where there are sequence values.\n",
    "            y_i[:res['seq_len'].values[i]] = res['Y'].values[i]\n",
    "        \n",
    "        return x, y, res['seq_len']\n",
    "    \n",
    "# EXAMPLE\n",
    "d = FullPaddedDataIterator(df_train)\n",
    "t = d.next_batch(1)\n",
    "\n",
    "print 'X sequences:\\n', t[0]\n",
    "print 'Y sequences:\\n', t[1]\n",
    "print 'sequence lengths:\\n', t[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BUILD COMPUTATIONAL GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HOUSE CLEANING\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "num_epochs = 2\n",
    "state_size = 100 # i.e. dimension of hidden layer.\n",
    "num_classes = label_vocabulary_size\n",
    "batch_size = 100\n",
    "\n",
    "# DEFINE GRAPH\n",
    "\n",
    "# Inputs\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, max_len])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "# Projection to embedding space\n",
    "W = tf.Variable(np.random.rand(1+state_size, state_size), dtype=tf.float32) # 1+state_size: one-hot size + hidden size.\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "# Projection to output space\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack data to slices (shape=(batch_size,1), 1 for one-hot size) across batch members\n",
    "#   cf. tf.unpack?\n",
    "inputs_series = tf.unpack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unpack(batchY_placeholder, axis=1)\n",
    "# ALTERNATIVE SOL: format data such that Tensorflow cell can create state-state chain\n",
    "#   only used when also using Tensorflow's cell\n",
    "# inputs_series = tf.split(1, max_len, batchX_placeholder)\n",
    "\n",
    "# Build RNN pipeline: state-state concatenation\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat(1, [current_input, current_state])\n",
    "    # projecting state and one-hot input at the same time.\n",
    "    # NB: broadcasted addition.\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state\n",
    "# ALTERNATIVE SOL: using Tensorflow's ready-made\n",
    "#   must be used with formatted input described above\n",
    "# cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "# states_series, current_state = tf.nn.rnn(cell, inputs_series, init_state)\n",
    "\n",
    "# Forward pass\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] \n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "# Evaluation\n",
    "correct = [tf.equal(tf.argmax(pred,1), tf.cast(true,tf.int64)) \n",
    "           for pred,true in zip(predictions_series, labels_series)]\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# Loss function\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) \n",
    "          for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# Set training method\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRAINING & EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create data readers\n",
    "    tr = FullPaddedDataIterator(df_train)\n",
    "    te = FullPaddedDataIterator(df_test)\n",
    "    # Record keepers\n",
    "    tr_losses, te_losses = [], []\n",
    "    tr_accuracies, te_accuracies = [], []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    # Training\n",
    "    init_state_ = np.zeros((batch_size, state_size))\n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,_ = tr.next_batch(batch_size) # _ for sequence length. not used yet.\n",
    "        total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "            [total_loss, train_step, init_state, accuracy],\n",
    "            feed_dict = {batchX_placeholder:tr_x,\n",
    "                         batchY_placeholder:tr_y,\n",
    "                         init_state:init_state_}\n",
    "            )\n",
    "        tr_losses.append(total_loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 100 == 0:\n",
    "            print \"Avg Training Loss at\", step, ':', np.mean(tr_losses)\n",
    "            print \"Avg Training Accuracy at\", step, ':', np.mean(tr_accuracies)\n",
    "            te_x,te_y,_ = te.next_batch(batch_size) # randomly sample 100 to evaluate.\n",
    "            total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "                [total_loss, train_step, init_state, accuracy],\n",
    "                feed_dict = {batchX_placeholder:te_x,\n",
    "                             batchY_placeholder:te_y,\n",
    "                             init_state:init_state_}\n",
    "                )\n",
    "            te_losses.append(total_loss_)\n",
    "            te_accuracies.append(accuracy_)\n",
    "            print \"Avg Test Loss at\", step, ':', np.mean(te_losses)\n",
    "            print \"Avg Test Accuracy at\", step, ':', np.mean(te_accuracies)\n",
    "            print\n",
    "        if tr.epochs > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. SINGLE-LAYER LSTM \n",
    "\n",
    "* Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HOUSE CLEANING\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "num_epochs = 2\n",
    "state_size = 100 # i.e. dimension of hidden layer.\n",
    "num_classes = label_vocabulary_size\n",
    "batch_size = 100\n",
    "\n",
    "# DEFINE GRAPH\n",
    "\n",
    "# Inputs\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, max_len])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "\n",
    "# LSTM states\n",
    "#   cell state: memory cell.\n",
    "#   hidden state: just the same hidden layer as in simple RNN.\n",
    "cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "# Projection to output space\n",
    "#   NB: these are same as W2, b2 above. now the to-embedding projection is automatic\n",
    "W = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack data to slices (shape=(batch_size,1), 1 for one-hot size) across batch members\n",
    "#   cf. tf.unpack?\n",
    "inputs_series = tf.split(1, max_len, batchX_placeholder)\n",
    "labels_series = tf.unpack(batchY_placeholder, axis=1)\n",
    "# Build RNN pipeline: state-state concatenation\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "states_series, current_state = tf.nn.rnn(cell, inputs_series, init_state)\n",
    "\n",
    "# Forward pass\n",
    "logits_series = [tf.matmul(state, W) + b for state in states_series] \n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "# Evaluation\n",
    "correct = [tf.equal(tf.argmax(pred,1), tf.cast(true,tf.int64)) \n",
    "           for pred,true in zip(predictions_series, labels_series)]\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# Loss function\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) \n",
    "          for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# Set training method\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create data readers\n",
    "    tr = FullPaddedDataIterator(df_train)\n",
    "    te = FullPaddedDataIterator(df_test)\n",
    "    # Record keepers\n",
    "    tr_losses, te_losses = [], []\n",
    "    tr_accuracies, te_accuracies = [], []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    # Training\n",
    "    cell_state_ = np.zeros((batch_size, state_size))\n",
    "    hidden_state_ = np.zeros((batch_size, state_size))\n",
    "    \n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,_ = tr.next_batch(batch_size) # _ for sequence length. not used yet.\n",
    "        total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "            [total_loss, train_step, init_state, accuracy],\n",
    "            feed_dict = {batchX_placeholder:tr_x,\n",
    "                         batchY_placeholder:tr_y,\n",
    "                         cell_state:cell_state_,\n",
    "                         hidden_state:hidden_state_}\n",
    "            )\n",
    "        tr_losses.append(total_loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 100 == 0:\n",
    "            print \"Avg Training Loss at\", step, ':', np.mean(tr_losses)\n",
    "            print \"Avg Training Accuracy at\", step, ':', np.mean(tr_accuracies)\n",
    "            te_x,te_y,_ = te.next_batch(batch_size) # randomly sample 100 to evaluate.\n",
    "            total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "                [total_loss, train_step, init_state, accuracy],\n",
    "                feed_dict = {batchX_placeholder:te_x,\n",
    "                             batchY_placeholder:te_y,\n",
    "                             cell_state:cell_state_,\n",
    "                             hidden_state:hidden_state_}\n",
    "                )\n",
    "            te_losses.append(total_loss_)\n",
    "            te_accuracies.append(accuracy_)\n",
    "            print \"Avg Test Loss at\", step, ':', np.mean(te_losses)\n",
    "            print \"Avg Test Accuracy at\", step, ':', np.mean(te_accuracies)\n",
    "            print\n",
    "        if tr.epochs > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. BI-SLTM\n",
    "\n",
    "* Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HOUSE CLEANING\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "num_epochs = 2\n",
    "state_size = 100 # i.e. dimension of hidden layer.\n",
    "num_classes = label_vocabulary_size\n",
    "batch_size = 100\n",
    "\n",
    "# DEFINE GRAPH\n",
    "\n",
    "# Inputs\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, max_len])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "\n",
    "# LSTM states\n",
    "#   cell state: memory cell.\n",
    "#   hidden state: just the same hidden layer as in simple RNN.\n",
    "#   fwd & bwd need to both take the same init.\n",
    "cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state) \n",
    "    # NB: init_state per se is not the tensor to be trained\n",
    "    #     it is cell_state & hidden_state!\n",
    "\n",
    "# Projection to output space\n",
    "#   NB: these are same as W2, b2 above. now the to-embedding projection is automatic by Tensorflow.\n",
    "W = tf.Variable(np.random.rand(state_size*2, num_classes),dtype=tf.float32) # *2: combined weights for fwd & bwd.\n",
    "b = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack data to slices (shape=(batch_size,1), 1 for one-hot size) across batch members\n",
    "#   cf. tf.unpack?\n",
    "inputs_series = tf.split(1, max_len, batchX_placeholder)\n",
    "labels_series = tf.unpack(batchY_placeholder, axis=1)\n",
    "# Build Bi-LSTM pipeline: state-state concatenation\n",
    "with tf.variable_scope('Bi-LSTM'): # not a must. added for the convenience of tensorboard visualization.\n",
    "    fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "    bwd_cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "    states_series, fwd_current_state, bwd_current_state = tf.nn.bidirectional_rnn(fwd_cell, bwd_cell, inputs_series, \n",
    "                                                                                  initial_state_fw=init_state,\n",
    "                                                                                  initial_state_bw=init_state)\n",
    "# Forward pass\n",
    "logits_series = [tf.matmul(state, W) + b for state in states_series] \n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "# Evaluation\n",
    "correct = [tf.equal(tf.argmax(pred,1), tf.cast(true,tf.int64)) \n",
    "           for pred,true in zip(predictions_series, labels_series)]\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# Loss function\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) \n",
    "          for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# Set training method\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create data readers\n",
    "    tr = FullPaddedDataIterator(df_train)\n",
    "    te = FullPaddedDataIterator(df_test)\n",
    "    # Record keepers\n",
    "    tr_losses, te_losses = [], []\n",
    "    tr_accuracies, te_accuracies = [], []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    # Training\n",
    "    cell_state_ = np.zeros((batch_size, state_size))\n",
    "    hidden_state_ = np.zeros((batch_size, state_size))\n",
    "    \n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,_ = tr.next_batch(batch_size) # _ for sequence length. not used yet.\n",
    "        total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "            [total_loss, train_step, init_state, accuracy],\n",
    "            feed_dict = {batchX_placeholder:tr_x,\n",
    "                         batchY_placeholder:tr_y,\n",
    "                         cell_state:cell_state_,\n",
    "                         hidden_state:hidden_state_}\n",
    "            ) # NB: in decoding bidirectional networks, ALWAYS feed cell_state & hidden_state rather than init_state!\n",
    "        tr_losses.append(total_loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 100 == 0:\n",
    "            print \"Avg Training Loss at\", step, ':', np.mean(tr_losses)\n",
    "            print \"Avg Training Accuracy at\", step, ':', np.mean(tr_accuracies)\n",
    "            te_x,te_y,_ = te.next_batch(batch_size) # randomly sample 100 to evaluate.\n",
    "            total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "                [total_loss, train_step, init_state, accuracy],\n",
    "                feed_dict = {batchX_placeholder:te_x,\n",
    "                             batchY_placeholder:te_y,\n",
    "                             cell_state:cell_state_,\n",
    "                             hidden_state:hidden_state_}\n",
    "                )\n",
    "            te_losses.append(total_loss_)\n",
    "            te_accuracies.append(accuracy_)\n",
    "            print \"Avg Test Loss at\", step, ':', np.mean(te_losses)\n",
    "            print \"Avg Test Accuracy at\", step, ':', np.mean(te_accuracies)\n",
    "            print\n",
    "        if tr.epochs > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. MULTI-LAYER LSTM\n",
    "\n",
    "* Full Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HOUSE CLEANING\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "num_epochs = 2\n",
    "state_size = 100 # i.e. dimension of hidden layer.\n",
    "num_classes = label_vocabulary_size\n",
    "batch_size = 100\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "# DEFINE GRAPH\n",
    "\n",
    "# Inputs\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, max_len])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "\n",
    "# LSTM states\n",
    "#   2: for cell and hidden states.\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "state_per_layer_list = tf.unpack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)]\n",
    ") # distribute cell_state, hidden state to each layer.\n",
    "\n",
    "# Projection to output space\n",
    "#   NB: these are same as W2, b2 above. now the to-embedding projection is automatic by Tensorflow.\n",
    "W = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack data to slices (shape=(batch_size,1), 1 for one-hot size) across batch members\n",
    "#   cf. tf.unpack?\n",
    "inputs_series = tf.split(1, max_len, batchX_placeholder)\n",
    "labels_series = tf.unpack(batchY_placeholder, axis=1)\n",
    "# Build RNN pipeline: state-state concatenation\n",
    "cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.rnn(cell, inputs_series, initial_state=rnn_tuple_state)\n",
    "\n",
    "# Forward pass\n",
    "logits_series = [tf.matmul(state, W) + b for state in states_series] \n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "# Evaluation\n",
    "correct = [tf.equal(tf.argmax(pred,1), tf.cast(true,tf.int64)) \n",
    "           for pred,true in zip(predictions_series, labels_series)]\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# Loss function\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) \n",
    "          for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# Set training method\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create data readers\n",
    "    tr = FullPaddedDataIterator(df_train)\n",
    "    # Record keepers\n",
    "    tr_losses = []\n",
    "    tr_accuracies = []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    # Training\n",
    "    init_state_ = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "    \n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,_ = tr.next_batch(batch_size) \n",
    "        total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "            [total_loss, train_step, init_state, accuracy],\n",
    "            feed_dict = {batchX_placeholder:tr_x,\n",
    "                         batchY_placeholder:tr_y,\n",
    "                         init_state:init_state_}\n",
    "            )        \n",
    "        tr_losses.append(total_loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 100 == 0:\n",
    "            print \"Avg Training Loss at\", step, ':', np.mean(tr_losses)\n",
    "            print \"Avg Training Accuracy at\", step, ':', np.mean(tr_accuracies)\n",
    "            te_losses = []\n",
    "            te_accuracies = []\n",
    "            te = FullPaddedDataIterator(df_test)\n",
    "            te.shuffle()\n",
    "            while te.epochs==0:\n",
    "                te_x,te_y,_ = te.next_batch(batch_size) # randomly sample 100 to evaluate.\n",
    "                total_loss_, init_state_, accuracy_ = sess.run(\n",
    "                    [total_loss, init_state, accuracy],\n",
    "                    feed_dict = {batchX_placeholder:te_x,\n",
    "                                 batchY_placeholder:te_y,\n",
    "                                 init_state:init_state_}\n",
    "                    )\n",
    "                te_losses.append(total_loss_)\n",
    "                te_accuracies.append(accuracy_)\n",
    "            print \"Avg Test Loss at\", step, ':', np.mean(te_losses)\n",
    "            print \"Avg Test Accuracy at\", step, ':', np.mean(te_accuracies)\n",
    "            print            \n",
    "        if tr.epochs > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Implementation 4e\n",
    "\n",
    "* NB: same data iterator as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. DYNAMIC MULTI-LAYER LSTM\n",
    "\n",
    "* Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HOUSE CLEANING\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "num_epochs = 2\n",
    "state_size = 100 # i.e. dimension of hidden layer.\n",
    "num_classes = label_vocabulary_size\n",
    "batch_size = 100\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "# DEFINE GRAPH\n",
    "\n",
    "# Inputs\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, max_len])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "batchSeqlen_placeholder = tf.placeholder(tf.int32, [batch_size])\n",
    "\n",
    "# LSTM states\n",
    "#   2: for cell and hidden states.\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "state_per_layer_list = tf.unpack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)]\n",
    ") # distribute cell_state, hidden state to each layer.\n",
    "\n",
    "# Projection to output space\n",
    "#   NB: these are same as W2, b2 above. now the to-embedding projection is automatic by Tensorflow.\n",
    "W = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack data to slices (shape=(batch_size,1), 1 for one-hot size) across batch members\n",
    "#   cf. tf.unpack?\n",
    "# Build RNN pipeline: state-state concatenation\n",
    "cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "# Dynamic RNN architecture\n",
    "#   tf.expand_dims(batchX_placeholder, -1): dynamic_rnn takes input with [batch_size, max_len, input_size],\n",
    "#                                           we thus add 1 extra dimension [input_size].\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, tf.expand_dims(batchX_placeholder, -1),\n",
    "                                                 sequence_length=batchSeqlen_placeholder,\n",
    "                                                 initial_state=rnn_tuple_state)\n",
    "states_series = tf.reshape(states_series, [-1, state_size]) # size squeezed to [batch_size*max_len, state_size].\n",
    "\n",
    "# Forward pass \n",
    "logits = tf.matmul(states_series, W) + b\n",
    "labels = tf.reshape(batchY_placeholder, [-1]) # make labels into one long list.\n",
    "logits_series = tf.unpack(tf.reshape(logits, [batch_size, max_len, num_classes]), axis=1)\n",
    "predictions_series = [tf.nn.softmax(logit) for logit in logits_series]\n",
    "# Evaluation\n",
    "preds_list = [tf.argmax(pred,1) for pred in predictions_series]\n",
    "preds_concat = tf.concat(0, preds_list) # concatenate batch predictions into one list to match the format of labels.\n",
    "correct = tf.equal(preds_concat, tf.cast(labels,tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# Loss function\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# Set training method\n",
    "train_step = tf.train.AdagradOptimizer(1e-4).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create data readers\n",
    "    tr = FullPaddedDataIterator(df_train)\n",
    "    te = FullPaddedDataIterator(df_test)\n",
    "    # Record keepers\n",
    "    tr_losses, te_losses = [], []\n",
    "    tr_accuracies, te_accuracies = [], []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    # Training\n",
    "    init_state_ = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "    \n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,tr_seqlen = tr.next_batch(batch_size)\n",
    "        total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "            [total_loss, train_step, init_state, accuracy],\n",
    "            feed_dict = {batchX_placeholder:tr_x,\n",
    "                         batchY_placeholder:tr_y,\n",
    "                         batchSeqlen_placeholder:tr_seqlen,\n",
    "                         init_state:init_state_}\n",
    "            )        \n",
    "        tr_losses.append(total_loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 100 == 0:\n",
    "            print \"Avg Training Loss at\", step, ':', np.mean(tr_losses)\n",
    "            print \"Avg Training Accuracy at\", step, ':', np.mean(tr_accuracies)\n",
    "            te_x,te_y,te_seqlen = te.next_batch(batch_size) # randomly sample 100 to evaluate.\n",
    "            total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "                [total_loss, train_step, init_state, accuracy],\n",
    "                feed_dict = {batchX_placeholder:te_x,\n",
    "                             batchY_placeholder:te_y,\n",
    "                             batchSeqlen_placeholder:te_seqlen,\n",
    "                             init_state:init_state_}\n",
    "                )\n",
    "            te_losses.append(total_loss_)\n",
    "            te_accuracies.append(accuracy_)\n",
    "            print \"Avg Test Loss at\", step, ':', np.mean(te_losses)\n",
    "            print \"Avg Test Accuracy at\", step, ':', np.mean(te_accuracies)\n",
    "            print\n",
    "        if tr.epochs > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. DYNAMIC BI-LSTM\n",
    "\n",
    "* Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HOUSE CLEANING\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "num_epochs = 2\n",
    "state_size = 100 # i.e. dimension of hidden layer.\n",
    "num_classes = label_vocabulary_size\n",
    "batch_size = 100\n",
    "\n",
    "# DEFINE GRAPH\n",
    "\n",
    "# Inputs\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, max_len])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "batchSeqlen_placeholder = tf.placeholder(tf.int32, [batch_size])\n",
    "\n",
    "# LSTM states\n",
    "#   cell state: memory cell.\n",
    "#   hidden state: just the same hidden layer as in simple RNN.\n",
    "#   fwd & bwd need to both take the same init.\n",
    "cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "# Projection to output space\n",
    "#   NB: these are same as W2, b2 above. now the to-embedding projection is automatic by Tensorflow.\n",
    "W = tf.Variable(np.random.rand(state_size*2, num_classes),dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack data to slices (shape=(batch_size,1), 1 for one-hot size) across batch members\n",
    "#   cf. tf.unpack?\n",
    "# Build Bi-LSTM pipeline: state-state concatenation\n",
    "fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "bwd_cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "# Dynamic RNN architecture\n",
    "#   tf.expand_dims(batchX_placeholder, -1): dynamic_rnn takes input with [batch_size, max_len, input_size],\n",
    "#                                           we thus add 1 extra dimension [input_size].\n",
    "states_series_tuple, current_states_tuple = tf.nn.bidirectional_dynamic_rnn(fwd_cell, bwd_cell, \n",
    "                                                                            tf.expand_dims(batchX_placeholder, -1),\n",
    "                                                                            sequence_length=batchSeqlen_placeholder,\n",
    "                                                                            initial_state_fw=init_state,\n",
    "                                                                            initial_state_bw=init_state)\n",
    "fwd_states_series, bwd_states_series = states_series_tuple\n",
    "fwd_states_series = tf.reshape(fwd_states_series, [-1, state_size]) # shape: [batch_size*max_len, state_size].\n",
    "bwd_states_series = tf.reshape(bwd_states_series, [-1, state_size])\n",
    "states_series = tf.concat(1, [fwd_states_series, bwd_states_series]) # concat along the dim of state_size.\n",
    "\n",
    "# Forward pass \n",
    "logits = tf.matmul(states_series, W) + b\n",
    "labels = tf.reshape(batchY_placeholder, [-1]) # make labels into one long list.\n",
    "logits_series = tf.unpack(tf.reshape(logits, [batch_size, max_len, num_classes]), axis=1)\n",
    "predictions_series = [tf.nn.softmax(logit) for logit in logits_series]\n",
    "# Evaluation\n",
    "preds_list = [tf.argmax(pred,1) for pred in predictions_series]\n",
    "preds_concat = tf.concat(0, preds_list) # concatenate batch predictions into one list to match the format of labels.\n",
    "correct = tf.equal(preds_concat, tf.cast(labels,tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# Loss function\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# Set training method\n",
    "train_step = tf.train.AdagradOptimizer(1e-4).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create data readers\n",
    "    tr = FullPaddedDataIterator(df_train)\n",
    "    te = FullPaddedDataIterator(df_test)\n",
    "    # Record keepers\n",
    "    tr_losses, te_losses = [], []\n",
    "    tr_accuracies, te_accuracies = [], []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    # Training\n",
    "    cell_state_ = np.zeros((batch_size, state_size))\n",
    "    hidden_state_ = np.zeros((batch_size, state_size))\n",
    "    \n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,_ = tr.next_batch(batch_size) # _ for sequence length. not used yet.\n",
    "        total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "            [total_loss, train_step, init_state, accuracy],\n",
    "            feed_dict = {batchX_placeholder:tr_x,\n",
    "                         batchY_placeholder:tr_y,\n",
    "                         batchSeqlen_placeholder:te_seqlen,\n",
    "                         cell_state:cell_state_,\n",
    "                         hidden_state:hidden_state_}\n",
    "            )\n",
    "        tr_losses.append(total_loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 100 == 0:\n",
    "            print \"Avg Training Loss at\", step, ':', np.mean(tr_losses)\n",
    "            print \"Avg Training Accuracy at\", step, ':', np.mean(tr_accuracies)\n",
    "            te_x,te_y,_ = te.next_batch(batch_size) # randomly sample 100 to evaluate.\n",
    "            total_loss_, train_step_, init_state_, accuracy_ = sess.run(\n",
    "                [total_loss, train_step, init_state, accuracy],\n",
    "                feed_dict = {batchX_placeholder:te_x,\n",
    "                             batchY_placeholder:te_y,\n",
    "                             batchSeqlen_placeholder:te_seqlen,\n",
    "                             cell_state:cell_state_,\n",
    "                             hidden_state:hidden_state_}\n",
    "                )\n",
    "            te_losses.append(total_loss_)\n",
    "            te_accuracies.append(accuracy_)\n",
    "            print \"Avg Test Loss at\", step, ':', np.mean(te_losses)\n",
    "            print \"Avg Test Accuracy at\", step, ':', np.mean(te_accuracies)\n",
    "            print\n",
    "        if tr.epochs > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
