{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning 03b. Recurrent Neural Nets (RNN)\n",
    "\n",
    "**NB**: This is a simple demo. Hyperparams are not tuned for optimal performance (1 epoch => 79% accuracy).\n",
    "\n",
    "**NB**: The code is developed with Tensorflow 1.0.0.\n",
    "\n",
    "* **Implementation 4d**: RNN with Tensorflow (Bi-LSTM with dynamic rnn)\n",
    "    * *Source*: \n",
    "        * Erik Hallstr&ouml;m's blog: https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185#.qg4y5kgbq.\n",
    "        * R2RT's blog: http://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html.\n",
    "    * *Contribution*:\n",
    "        * Both bloggers are brilliant, but neither did a simple POS-tagger example for non-technical linguists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, nltk, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.porter import PorterStemmer # a bit of text normalization to reduce vocab size.\n",
    "stemmer = PorterStemmer()\n",
    "from itertools import chain\n",
    "from spacy.en import English\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LODA DATA\n",
    "\n",
    "tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "N = len(tagged_sents)\n",
    "MAX_LEN = 100 # truncate sents longer than MAX_LEN, pad ones shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# Size Uniformization\n",
    "\n",
    "sents, taglists, seqlens = [], [], []\n",
    "\n",
    "def pad(words, padder):\n",
    "    num_words = len(words)\n",
    "    if num_words<MAX_LEN:\n",
    "        return words + padder*(MAX_LEN-num_words)\n",
    "    return words[:MAX_LEN]\n",
    "\n",
    "def text_normalize(words):\n",
    "    return [stemmer.stem(w.lower()) for w in words]\n",
    "\n",
    "for tagged_sent in tagged_sents:\n",
    "    words, tags = zip(*tagged_sent) # get tuples here\n",
    "    sents.append(pad(text_normalize(list(words)),padder=[' ']))\n",
    "    taglists.append(pad(list(tags),padder=['SPACE']))\n",
    "    seqlen = len(words)\n",
    "    seqlens.append(MAX_LEN if seqlen>MAX_LEN else seqlen)\n",
    "    \n",
    "# Train-Test Split    \n",
    "\n",
    "cutoff = int(N*.8)\n",
    "sents_train, taglists_train, seqlens_train = sents[:cutoff], taglists[:cutoff], seqlens[:cutoff]\n",
    "sents_test, taglists_test, seqlens_test = sents[cutoff:], taglists[cutoff:], seqlens[cutoff:]\n",
    "\n",
    "# Encoding in One-Hot Indices\n",
    "\n",
    "word_dic = {}\n",
    "tag_dic = {}\n",
    "\n",
    "def get_id(word, dic, unk=False):\n",
    "    if unk: return len(dic)\n",
    "    if word not in dic:\n",
    "        dic[word] = len(dic)\n",
    "    return dic[word]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, Seq_train, Seq_test = [], [], [], [], [], []\n",
    "\n",
    "for words,tags,seqlen in zip(sents_train,taglists_train,seqlens_train):\n",
    "    X_train.append([get_id(word,word_dic) for word in words])\n",
    "    Y_train.append([get_id(tag,tag_dic) for tag in tags])\n",
    "    Seq_train.append(seqlen)\n",
    "\n",
    "for words,tags,seqlen in zip(sents_test,taglists_test,seqlens_test):\n",
    "    X_test.append([get_id(word,word_dic) if word in word_dic else get_id(word,word_dic,unk=True) for word in words])\n",
    "    Y_test.append([get_id(tag,tag_dic) if tag in tag_dic else get_dic(tag,tag_dic,unk=True) for tag in tags])\n",
    "    Seq_test.append(seqlen) \n",
    "    \n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "Y_train, Y_test =  np.array(Y_train), np.array(Y_test)\n",
    "Seq_train, Seq_test = np.array(Seq_train), np.array(Seq_test)\n",
    "\n",
    "# Batch Data Feeder\n",
    "\n",
    "class DataIterator:\n",
    "    \n",
    "    def __init__(self, X, Y, Seq):\n",
    "        self.X = deepcopy(X)\n",
    "        self.Y = deepcopy(Y)\n",
    "        self.Seq = deepcopy(Seq)\n",
    "        self.size = len(X)\n",
    "        self.indices = np.arange(self.size)\n",
    "        self.epoch = 0\n",
    "        self.cursor = 0\n",
    "        self.shuffle()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.indices)\n",
    "        self.X = self.X[self.indices]\n",
    "        self.Y = self.Y[self.indices]\n",
    "        self.Seq = self.Seq[self.indices]\n",
    "        self.cursor = 0\n",
    "    \n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epoch += 1\n",
    "            self.shuffle()\n",
    "        X_batch = self.X[self.cursor:self.cursor+n]\n",
    "        Y_batch = self.Y[self.cursor:self.cursor+n]\n",
    "        Seq_batch = self.Seq[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return X_batch, Y_batch, Seq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BUILD COMPUTATIONAL GRAPH\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "# Clean existing graph before start.\n",
    "reset_graph()\n",
    "\n",
    "num_epochs  = 1\n",
    "hidden_size = 100 # i.e. dimension of hidden layer.\n",
    "vocab_size  = len(word_dic)+1\n",
    "num_classes = len(tag_dic)\n",
    "batch_size  = 32\n",
    "\n",
    "lr = 1e-4 # learning rate\n",
    "lmd = 0.01 # L2 regularization\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, MAX_LEN])\n",
    "Y = tf.placeholder(tf.int32, [None, MAX_LEN])\n",
    "Seq = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "embedding = tf.get_variable('embedding', [vocab_size,hidden_size], dtype=tf.float32)\n",
    "X_emb = tf.nn.embedding_lookup(embedding, tf.cast(X, tf.int32))\n",
    "\n",
    "fwd_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "bwd_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "\n",
    "outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fwd_cell,cell_bw=bwd_cell,inputs=X_emb,\n",
    "                                             dtype=tf.float32,sequence_length=Seq)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "outputs_concat = tf.nn.dropout(tf.concat(outputs, 2), keep_prob) # (batch_size,MAX_LEN,hidden_size)\n",
    "dense_input_size = int(outputs_concat.get_shape()[2]) # 100\n",
    "dense_inputs = tf.reshape(outputs_concat, [-1, dense_input_size]) # (batch_size*MAX_LEN,hidden_size)\n",
    "W = tf.get_variable('weight', shape=[dense_input_size, num_classes], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='bias')\n",
    "l2_loss = tf.constant(0.0)\n",
    "l2_loss += tf.nn.l2_loss(W)\n",
    "l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "logits = tf.nn.xw_plus_b(dense_inputs, W, b) # (?, num_classes)\n",
    "pred_probs = tf.nn.softmax(logits) # (?, num_classes)\n",
    "Y_pred = tf.cast(tf.argmax(pred_probs, dimension=1), tf.int32)\n",
    "Y_true = tf.reshape(Y, shape=[-1])\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_true)) + lmd*l2_loss\n",
    "\n",
    "correct = tf.cast(tf.equal(Y_pred, Y_true), tf.int32)\n",
    "mask = tf.cast(tf.not_equal(Y_true, tag_dic['SPACE']), tf.int32)\n",
    "total_seqlen = tf.cast(tf.reduce_sum(Seq), tf.float32)\n",
    "correct = tf.multiply(correct, mask)\n",
    "accuracy = tf.cast(tf.reduce_sum(correct), tf.float32) / total_seqlen\n",
    "\n",
    "train = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | loss: 2.68788 | acc: 0.234528\n",
      "Step 20 | loss: 2.68328 | acc: 0.236842\n",
      "Step 30 | loss: 2.67787 | acc: 0.241573\n",
      "Step 40 | loss: 2.67206 | acc: 0.246594\n",
      "Step 50 | loss: 2.66149 | acc: 0.249729\n",
      "Step 60 | loss: 2.65497 | acc: 0.231771\n",
      "Step 70 | loss: 2.64448 | acc: 0.240069\n",
      "Step 80 | loss: 2.62407 | acc: 0.226902\n",
      "Step 90 | loss: 2.59126 | acc: 0.275676\n",
      "Step 100 | loss: 2.59809 | acc: 0.234352\n",
      "Step 110 | loss: 2.58303 | acc: 0.240602\n",
      "Step 120 | loss: 2.58607 | acc: 0.229199\n",
      "Step 130 | loss: 2.57879 | acc: 0.266323\n",
      "Step 140 | loss: 2.57354 | acc: 0.25\n",
      "Step 150 | loss: 2.55386 | acc: 0.249001\n",
      "Step 160 | loss: 2.54894 | acc: 0.261429\n",
      "Step 170 | loss: 2.5739 | acc: 0.244403\n",
      "Step 180 | loss: 2.5413 | acc: 0.240811\n",
      "Step 190 | loss: 2.54706 | acc: 0.232947\n",
      "Step 200 | loss: 2.54842 | acc: 0.239227\n",
      "Step 210 | loss: 2.53342 | acc: 0.248196\n",
      "Step 220 | loss: 2.54667 | acc: 0.248696\n",
      "Step 230 | loss: 2.52893 | acc: 0.255474\n",
      "Step 240 | loss: 2.54491 | acc: 0.27518\n",
      "Step 250 | loss: 2.51877 | acc: 0.296584\n",
      "Step 260 | loss: 2.52705 | acc: 0.26\n",
      "Step 270 | loss: 2.48738 | acc: 0.302774\n",
      "Step 280 | loss: 2.52013 | acc: 0.288744\n",
      "Step 290 | loss: 2.52035 | acc: 0.265651\n",
      "Step 300 | loss: 2.49695 | acc: 0.289157\n",
      "Step 310 | loss: 2.49782 | acc: 0.281421\n",
      "Step 320 | loss: 2.48556 | acc: 0.289073\n",
      "Step 330 | loss: 2.4864 | acc: 0.286957\n",
      "Step 340 | loss: 2.49132 | acc: 0.273794\n",
      "Step 350 | loss: 2.45353 | acc: 0.300236\n",
      "Step 360 | loss: 2.50961 | acc: 0.298507\n",
      "Step 370 | loss: 2.47997 | acc: 0.285714\n",
      "Step 380 | loss: 2.4613 | acc: 0.288437\n",
      "Step 390 | loss: 2.45108 | acc: 0.32341\n",
      "Step 400 | loss: 2.43821 | acc: 0.329472\n",
      "Step 410 | loss: 2.46321 | acc: 0.321981\n",
      "Step 420 | loss: 2.47588 | acc: 0.346296\n",
      "Step 430 | loss: 2.45364 | acc: 0.324926\n",
      "Step 440 | loss: 2.42837 | acc: 0.300915\n",
      "Step 450 | loss: 2.46735 | acc: 0.355072\n",
      "Step 460 | loss: 2.44337 | acc: 0.341121\n",
      "Step 470 | loss: 2.42289 | acc: 0.356948\n",
      "Step 480 | loss: 2.42775 | acc: 0.367898\n",
      "Step 490 | loss: 2.41592 | acc: 0.38152\n",
      "Step 500 | loss: 2.43975 | acc: 0.379195\n",
      "Step 510 | loss: 2.40884 | acc: 0.434269\n",
      "Step 520 | loss: 2.35937 | acc: 0.415656\n",
      "Step 530 | loss: 2.42027 | acc: 0.485269\n",
      "Step 540 | loss: 2.39273 | acc: 0.447531\n",
      "Step 550 | loss: 2.39698 | acc: 0.475279\n",
      "Step 560 | loss: 2.36651 | acc: 0.509545\n",
      "Step 570 | loss: 2.37957 | acc: 0.465492\n",
      "Step 580 | loss: 2.38252 | acc: 0.55814\n",
      "Step 590 | loss: 2.35853 | acc: 0.499305\n",
      "Step 600 | loss: 2.3646 | acc: 0.545902\n",
      "Step 610 | loss: 2.33359 | acc: 0.544056\n",
      "Step 620 | loss: 2.33368 | acc: 0.582181\n",
      "Step 630 | loss: 2.35302 | acc: 0.569401\n",
      "Step 640 | loss: 2.34151 | acc: 0.556982\n",
      "Step 650 | loss: 2.30806 | acc: 0.6125\n",
      "Step 660 | loss: 2.30227 | acc: 0.580468\n",
      "Step 670 | loss: 2.30897 | acc: 0.588064\n",
      "Step 680 | loss: 2.30762 | acc: 0.6125\n",
      "Step 690 | loss: 2.30428 | acc: 0.590258\n",
      "Step 700 | loss: 2.27385 | acc: 0.643777\n",
      "Step 710 | loss: 2.32752 | acc: 0.595782\n",
      "Step 720 | loss: 2.30307 | acc: 0.639175\n",
      "Step 730 | loss: 2.28822 | acc: 0.615506\n",
      "Step 740 | loss: 2.3056 | acc: 0.644037\n",
      "Step 750 | loss: 2.27668 | acc: 0.667205\n",
      "Step 760 | loss: 2.20429 | acc: 0.667901\n",
      "Step 770 | loss: 2.20313 | acc: 0.635593\n",
      "Step 780 | loss: 2.22746 | acc: 0.669014\n",
      "Step 790 | loss: 2.23393 | acc: 0.662774\n",
      "Step 800 | loss: 2.29677 | acc: 0.640884\n",
      "Step 810 | loss: 2.26217 | acc: 0.649533\n",
      "Step 820 | loss: 2.22353 | acc: 0.661145\n",
      "Step 830 | loss: 2.17235 | acc: 0.690789\n",
      "Step 840 | loss: 2.23772 | acc: 0.65109\n",
      "Step 850 | loss: 2.27059 | acc: 0.638838\n",
      "Step 860 | loss: 2.17624 | acc: 0.674902\n",
      "Step 870 | loss: 2.22692 | acc: 0.65894\n",
      "Step 880 | loss: 2.1524 | acc: 0.681937\n",
      "Step 890 | loss: 2.22139 | acc: 0.657807\n",
      "Step 900 | loss: 2.15249 | acc: 0.668005\n",
      "Step 910 | loss: 2.20197 | acc: 0.676101\n",
      "Step 920 | loss: 2.21638 | acc: 0.648352\n",
      "Step 930 | loss: 2.16977 | acc: 0.702096\n",
      "Step 940 | loss: 2.16407 | acc: 0.675676\n",
      "Step 950 | loss: 2.16516 | acc: 0.70624\n",
      "Step 960 | loss: 2.21716 | acc: 0.681901\n",
      "Step 970 | loss: 2.16147 | acc: 0.67482\n",
      "Step 980 | loss: 2.15357 | acc: 0.705795\n",
      "Step 990 | loss: 2.12405 | acc: 0.70788\n",
      "Step 1000 | loss: 2.13588 | acc: 0.704871\n",
      "Step 1010 | loss: 2.11379 | acc: 0.677503\n",
      "Step 1020 | loss: 2.15649 | acc: 0.667634\n",
      "Step 1030 | loss: 2.15817 | acc: 0.660237\n",
      "Step 1040 | loss: 2.13703 | acc: 0.710683\n",
      "Step 1050 | loss: 2.15505 | acc: 0.666185\n",
      "Step 1060 | loss: 2.0403 | acc: 0.710857\n",
      "Step 1070 | loss: 2.05655 | acc: 0.696751\n",
      "Step 1080 | loss: 2.13972 | acc: 0.696375\n",
      "Step 1090 | loss: 2.0846 | acc: 0.730506\n",
      "Step 1100 | loss: 2.13085 | acc: 0.701515\n",
      "Step 1110 | loss: 2.15937 | acc: 0.722124\n",
      "Step 1120 | loss: 2.09376 | acc: 0.713467\n",
      "Step 1130 | loss: 2.12882 | acc: 0.698758\n",
      "Step 1140 | loss: 2.10943 | acc: 0.738931\n",
      "Step 1150 | loss: 2.07198 | acc: 0.760997\n",
      "Step 1160 | loss: 2.07923 | acc: 0.757991\n",
      "Step 1170 | loss: 2.07181 | acc: 0.72106\n",
      "Step 1180 | loss: 2.12726 | acc: 0.73928\n",
      "Step 1190 | loss: 2.06743 | acc: 0.7375\n",
      "Step 1200 | loss: 2.04159 | acc: 0.750663\n",
      "Step 1210 | loss: 2.04283 | acc: 0.767123\n",
      "Step 1220 | loss: 2.06316 | acc: 0.732394\n",
      "Step 1230 | loss: 2.13998 | acc: 0.734291\n",
      "Step 1240 | loss: 2.02016 | acc: 0.774497\n",
      "Step 1250 | loss: 2.04122 | acc: 0.753036\n",
      "Step 1260 | loss: 2.03623 | acc: 0.752368\n",
      "Step 1270 | loss: 2.06813 | acc: 0.762918\n",
      "Step 1280 | loss: 2.02888 | acc: 0.765583\n",
      "Step 1290 | loss: 2.05167 | acc: 0.768095\n",
      "Step 1300 | loss: 2.03693 | acc: 0.745205\n",
      "Step 1310 | loss: 2.0256 | acc: 0.781641\n",
      "Step 1320 | loss: 2.03935 | acc: 0.777444\n",
      "Step 1330 | loss: 2.00486 | acc: 0.741656\n",
      "Step 1340 | loss: 2.04626 | acc: 0.771386\n",
      "Step 1350 | loss: 2.0184 | acc: 0.77933\n",
      "Step 1360 | loss: 2.052 | acc: 0.796117\n",
      "Step 1370 | loss: 2.04972 | acc: 0.77259\n",
      "Step 1380 | loss: 1.99962 | acc: 0.82\n",
      "Step 1390 | loss: 1.91362 | acc: 0.791619\n",
      "Step 1400 | loss: 2.06299 | acc: 0.761364\n",
      "Step 1410 | loss: 2.0397 | acc: 0.812995\n",
      "Step 1420 | loss: 2.02178 | acc: 0.78979\n",
      "Step 1430 | loss: 2.02284 | acc: 0.797637\n",
      "\n",
      "Final Avg Training Loss: 2.30242\n",
      "Final Avg Training Accuracy: 0.535804\n",
      "Final Avg Test Loss: 2.10718\n",
      "Final Avg Test Accuracy: 0.792701\n",
      "CPU times: user 23min 30s, sys: 7min 42s, total: 31min 12s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAIN & EVALUATE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create data readers\n",
    "    tr = DataIterator(X_train,Y_train,Seq_train)\n",
    "    te = DataIterator(X_test,Y_test,Seq_test)\n",
    "    \n",
    "    # Record keepers\n",
    "    tr_losses, te_losses = [], []\n",
    "    tr_accuracies, te_accuracies = [], []    \n",
    "    step = 0\n",
    "    current_epoch = 0\n",
    "    \n",
    "    # Training\n",
    "    while current_epoch < num_epochs:\n",
    "        step += 1\n",
    "        tr_x,tr_y,tr_seq = tr.next_batch(batch_size)\n",
    "        loss_, train_, accuracy_ = sess.run([loss, train, accuracy],\n",
    "                                             feed_dict = {X:tr_x,\n",
    "                                                          Y:tr_y,\n",
    "                                                          Seq:tr_seq,\n",
    "                                                          keep_prob:0.75})\n",
    "        tr_losses.append(loss_)\n",
    "        tr_accuracies.append(accuracy_)\n",
    "        if step % 10 == 0:\n",
    "            print \"Step\", step, \"| loss:\", loss_, \"| acc:\", accuracy_\n",
    "        if tr.epoch > current_epoch: # go to the next epoch.\n",
    "            current_epoch += 1\n",
    "            step = 0\n",
    "            \n",
    "    # Test       \n",
    "    te_x,te_y,te_seq = te.next_batch(len(X_test))\n",
    "    loss_, accuracy_ = sess.run([loss, accuracy],\n",
    "                                 feed_dict = {X:te_x,\n",
    "                                              Y:te_y,\n",
    "                                              Seq:te_seq,\n",
    "                                              keep_prob:1.0})\n",
    "    te_losses.append(loss_)\n",
    "    te_accuracies.append(accuracy_)\n",
    "    print\n",
    "    print \"Final Avg Training Loss:\", np.mean(tr_losses)\n",
    "    print \"Final Avg Training Accuracy:\", np.mean(tr_accuracies)   \n",
    "    print \"Final Avg Test Loss:\", np.mean(te_losses)\n",
    "    print \"Final Avg Test Accuracy:\", np.mean(te_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
